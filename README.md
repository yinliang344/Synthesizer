# Synthesizer
a new function for sequence。  
根据自己的理解，代码实现了synthesizer。  
有些地方可能理解不到位。  
# 疑惑的地方
1、为什么隐藏层的维度要和序列长度一致？在transformer中并不要求隐藏的长度。  
2、低秩分解random synthesizer模型中，要求两个矩阵的最后一个维度相同吗？具体的处理方法是什么？  
3、文章说支持多头注意力机制，但是有些地方并没有想明白如何支持。若干个不同的注意力方法加权在一起，是否可以看成另一种多头机制？   
4、关于参数膨胀的问题：按照论文的说法，想要支持多头注意力机制，论文中所提到的四个方法必须在初始化的B的时候最后一维扩大nb_head倍（因为没有dot操作），再将矩阵分解为四维矩阵，利用矩阵乘法进行多头同时操作，但是这样无疑中会增加成倍的参数，暂时没想到好的解决办法。
